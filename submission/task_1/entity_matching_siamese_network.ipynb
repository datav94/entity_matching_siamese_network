{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTITY MATCHING EXPERIMENT\n",
    "\n",
    "#### Data description\n",
    "\n",
    "The data provided is in csv format and contains two features\n",
    "and a label. The unnamed first column has been analyzed and\n",
    "it contains only unique values with certain values missing from the sequence thereby\n",
    "establishing the following assumption.\n",
    "\n",
    "Assumption: The unnamed first column is the index column where the\n",
    "missing values from the sequence indicate a data cleaning task \n",
    "performed prior to handing over the dataset\n",
    "\n",
    "Note: The above assumption does not rule out the further need for\n",
    "data cleaning and preprocessing. It is only to take the unnamed column\n",
    "as index for the dataset.\n",
    "\n",
    "The following are the features of the dataset:\n",
    "\n",
    "\"entity_1\" : This contains company names with extra spaces and string length as low as 1\n",
    "\"entity_2\" : This contains company names with extra spaces and string length as low as 1\n",
    "\n",
    "The label column in the dataset is named \"tag\" and it is a binary integar data with values 1 and 0\n",
    "\n",
    "The data set is imbalanced with the negative class accounting to 60% of the examples\n",
    "and positive class accounting to 40% of the example\n",
    "\n",
    "There are no missing values in any of the features\n",
    "\n",
    "\n",
    "#### Data Cleaning process\n",
    "\n",
    "\n",
    "#### Balancing the dataset\n",
    "\n",
    "\n",
    "#### Model Selection (Answer to the first part of task 1)\n",
    "\n",
    "There are various ways in which this task can be handled\n",
    "as described below:\n",
    "\n",
    "1. Siamese Networks\n",
    "2. Attention mechanisms\n",
    "3. Transformers\n",
    "4. Pre-trained models like BERT for generating embeddings and vector space\n",
    "\n",
    "The above list is not exhaustive and hence one can take advantage\n",
    "of various new research like the one suggested in the paper titled\n",
    "\"Business Entity Matching with Siamese Graph Convolutional Networks\"\n",
    "written by employees and trainees at IBM Zurich. The link for which is given below\n",
    "\n",
    "https://arxiv.org/abs/2105.03701\n",
    "\n",
    "This architecture uses BERT + GCN and according to the paper it generates\n",
    "better results in terms of robustness to semantic meanings and title endings etc\n",
    "\n",
    "The model selected for this task is a Siamese Network. The Siamese network\n",
    "is an architecture that is highly popular in tasks where similarity is to be \n",
    "measured between pairs of input data.\n",
    "\n",
    "The key idea is to have two identical subnetworks that share same parameters,\n",
    "weights and biases a.k.a the siamese twins\n",
    "\n",
    "This architecture when developed objectivises robustness to variation while learning\n",
    "meaningful representations to the inputs\n",
    "\n",
    "The distance metric used here is a L1 norm since it is more robust\n",
    "\n",
    "The model architecture presented here consist of an embedding layer,\n",
    "followed by shared LSTM units followed by L1 distance.\n",
    " This is followed by fully connected layers\n",
    "and dropouts for regularization thereby avoiding overfitting.\n",
    "\n",
    "LSTM is chosen for the shared architecture as it is highly efficient\n",
    "in capturing long term dependencies and relation ships in data such as company names\n",
    "\n",
    "The order of words is quite crucial in company names and this is effectively captured by an LSTM unit\n",
    "\n",
    "Embedding layers capture semantic meanings between words by representing them in\n",
    "continuous vector space. This can be helpful to capture the subtleties in company names\n",
    "\n",
    "The final prediction involves usage of a sigmoid since we are performing binary classification\n",
    "\n",
    "Loss is calculated using binary cross entropy as done for various binary classification problems\n",
    "\n",
    "Optimizer used: Adam\n",
    "\n",
    "No Hyperparameter tuning has been performed as it is not required for the given dataset and problem\n",
    "but if the dataset increases in size or while putting the model to production this may be completely\n",
    "necessary.\n",
    "\n",
    "There are a few downsides to this architecture as it is complex and requires heavy computational\n",
    "resources. It requires large amount of data and tuning it for hyperparameter becomes crucial\n",
    "despite the computation expense.\n",
    "\n",
    "It can be difficult to interpret the internal working of model related to how it makes decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries and dependencies\n",
    "\n",
    "# python's batteries included libraries\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "# for data processing and EDA\n",
    "import spacy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# viz \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from the prescribed path\n",
    "\n",
    "def load_data(file_name, index_col=\"Unnamed: 0\"):\n",
    "    \"\"\"\n",
    "        There is a prior assumption that the\n",
    "        dataset is stored under the \"data\"\n",
    "        directory in the current working directory\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO: Add validation to check the Unnamed: 0 col as duplicated or drop\n",
    "    return pd.read_csv(\n",
    "        Path().cwd() / \"data\" / file_name,\n",
    "        index_col=index_col\n",
    "    )\n",
    "\n",
    "df = load_data(\"ds_challenge_alpas_indexed.csv\")\n",
    "\n",
    "# df.to_csv(Path().cwd() / \"data\" / \"ds_challenge_alpas_indexed.csv\", index_label=\"index_col\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_col</th>\n",
       "      <th>entity_1</th>\n",
       "      <th>entity_2</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3137667</td>\n",
       "      <td>preciform A.B</td>\n",
       "      <td>Preciform AB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5515816</td>\n",
       "      <td>degener staplertechnik vertriebs-gmbh</td>\n",
       "      <td>Irshim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215797</td>\n",
       "      <td>Alltel South CaroliNA Inc</td>\n",
       "      <td>alltel south carolina INC.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004621</td>\n",
       "      <td>cse Corporation</td>\n",
       "      <td>Cse Corp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1698689</td>\n",
       "      <td>Gruppo D Motors Srl</td>\n",
       "      <td>gruppo d motors Sociedad de Resposabilidad Lim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_col                               entity_1  \\\n",
       "0    3137667                          preciform A.B   \n",
       "1    5515816  degener staplertechnik vertriebs-gmbh   \n",
       "2     215797              Alltel South CaroliNA Inc   \n",
       "3    1004621                        cse Corporation   \n",
       "4    1698689                    Gruppo D Motors Srl   \n",
       "\n",
       "                                            entity_2  tag  \n",
       "0                                       Preciform AB    1  \n",
       "1                                             Irshim    0  \n",
       "2                         alltel south carolina INC.    1  \n",
       "3                                           Cse Corp    1  \n",
       "4  gruppo d motors Sociedad de Resposabilidad Lim...    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    59.095513\n",
       "1    40.904487\n",
       "Name: tag, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution in the labels\n",
    "\n",
    "df.tag.value_counts() / df.tag.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7042846, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7042846 entries, 0 to 7042845\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   index_col  int64 \n",
      " 1   entity_1   object\n",
      " 2   entity_2   object\n",
      " 3   tag        int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 214.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The data cleaning process covers the following tasks\n",
    "\n",
    "1. Removing Punctuations\n",
    "2. Lower casing the strings\n",
    "3. Stripping extra spaces\n",
    "4. Deduplication of records\n",
    "5. Removing records where both the features have length less than or equal to 3 \n",
    "\n",
    "(This does not provide any sementic meaning for company names thereby leading to learning unwanted patterns for the model and hence need to be removed. The quantity of such data is considerably low and can be ignored if more robust models like S-GCN architecture are to be constructed)\n",
    "\n",
    "even after the data cleaning there are no missing values or empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing punctuations from entity_1\n",
      "lower casing entity_1\n",
      "removing punctuations from entity_2\n",
      "lower casing entity_2\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and make everything lower case\n",
    "# re.escape used to put escape char wherever necessary\n",
    "\n",
    "def preprocess_data(cols):\n",
    "    df_cols = df.columns\n",
    "\n",
    "    for col in cols: assert col in df_cols, f\"{col} not in dataset column names\"\n",
    "\n",
    "    for col in cols:\n",
    "        print(f\"removing punctuations from {col}\")\n",
    "        df[col] = df[col].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "        print(f\"lower casing {col}\")\n",
    "        df[col] = df[col].apply(lambda x: x.lower().strip())\n",
    "\n",
    "preprocess_data([\"entity_1\", \"entity_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4162006\n",
       "1    2880840\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# de duplicate all records\n",
    "def deduplicated_records(df, strategy=False):\n",
    "\n",
    "    duplicate_mask = df.duplicated(keep=strategy)\n",
    "\n",
    "    return df[~duplicate_mask]\n",
    "\n",
    "\n",
    "df_deduplicated = deduplicated_records(df)\n",
    "df_deduplicated.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index_col    0\n",
       "entity_1     0\n",
       "entity_2     0\n",
       "tag          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deduplicated.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3521423, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deduplicated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short company names\n",
    "\n",
    "def remove_short_names(df, ln=2):\n",
    "    mask = (df.entity_1.str.len() > ln) & (df.entity_2.str.len() > ln)\n",
    "    return df[mask]\n",
    "\n",
    "df_clean = remove_short_names(df_deduplicated, ln=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the dataset\n",
    "\n",
    "The dataset can be easily balanced by using imblearn.over_sampling.SMOTE\n",
    "The balancing strategy according to the commented logic below is as fellows:\n",
    "\n",
    "1. We use SMOTE to over sample the dataset thereby increasing its size\n",
    "2. Further we use RandomUnderSampler to under sample the dataset in order to bring it back to approximately its original size\n",
    "\n",
    "Performing over sampling is compute expensive\n",
    "for local runs. Hence Right now we shall\n",
    "select and balance the dataset manual for trial runs\n",
    "This introduces a selection bias in the dataset\n",
    "But nevertheless, we can perform SMOTE or SMOTE-ENN\n",
    "or SMOTE with RandomUnderSampler to bring back the\n",
    "dataset size approx to the original.\n",
    "\n",
    "This can be done with distributed training in Apache beam\n",
    "A sample pipeline for which has been developed along with this\n",
    "Submission.\n",
    "\n",
    "Caution: The Apache Beam pipeline is highly error prone and requires debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to balance the dataset using\n",
    "# Synthetic Minority Oversampling Technique\n",
    "# SMOTE-ENN can also be used here with embeddings\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# X_train = X_train.entity_1 + \"|\" + X_train.entity_2\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# oversample = SMOTE(random_state=42)\n",
    "# X_bal, y_bal = oversample.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# X_bal = X_bal.str.split(\"|\", expand=True)\n",
    "\n",
    "# # Rename the cols as entity_1 and entity_2\n",
    "# # prepare the final X_train using pandas transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Performing over sampling is compute expensive\n",
    "    for local runs. Hence Right now we shall\n",
    "    select and balance the dataset manual for trial runs\n",
    "    This introduces a selection bias in the dataset\n",
    "    But nevertheless, we can perform SMOTE or SMOTE-ENN\n",
    "    or SMOTE with RandomUnderSampler to bring back the\n",
    "    dataset size approx to the original.\n",
    "\n",
    "    This can be done with distributed training in Apache beam\n",
    "    A sample pipeline for which has been developed along with this\n",
    "    Submission.\n",
    "\n",
    "    Caution: The Apache Beam pipeline is highly error prone and requires debugging\n",
    "\"\"\"\n",
    "\n",
    "# Class based manual balancing of data set introducing selection bias\n",
    "df_sample = pd.concat(\n",
    "    [df_clean[(df_clean.tag == 0)][:25000], df_clean[(df_clean.tag == 1)][:25000]],\n",
    ")\n",
    "\n",
    "df_sample.tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train test split \n",
    "\n",
    "A more efficient splitting can be employed using tfx pipelines as described in tfx pipeline files\n",
    "\n",
    "### Preprocessing Model Inputs\n",
    "\n",
    "Preprocessing model inputs involves tokenizing,\n",
    "encoding and padding the inputs as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample[[\"entity_1\", \"entity_2\"]], df_sample.tag, test_size=0.33, random_state=42)\n",
    "\n",
    "labeled_data = X_train\n",
    "\n",
    "\n",
    "# Process the labeled data\n",
    "def preprocess_model_inputs(labeled_data):\n",
    "    # company_pairs = list(zip(labeled_data['entity_1'], labeled_data['entity_2']))\n",
    "    # labels = np.array(y_train)\n",
    "\n",
    "    # Tokenize and pad the company names\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(list(labeled_data['entity_1']) + list(labeled_data['entity_2']))\n",
    "\n",
    "    # encoding company names\n",
    "    encoded_company1 = tokenizer.texts_to_sequences(list(labeled_data['entity_1']))\n",
    "    encoded_company2 = tokenizer.texts_to_sequences(list(labeled_data['entity_2']))\n",
    "\n",
    "    max_length = max(max(len(seq) for seq in encoded_company1), max(len(seq) for seq in encoded_company2))\n",
    "\n",
    "    # applying padding to gain equal size for all examples\n",
    "    padded_company1 = tf.keras.preprocessing.sequence.pad_sequences(encoded_company1, maxlen=max_length)\n",
    "    padded_company2 = tf.keras.preprocessing.sequence.pad_sequences(encoded_company2, maxlen=max_length)\n",
    "\n",
    "    return padded_company1, padded_company2, tokenizer, max_length\n",
    "\n",
    "padded_company1, padded_company2, tokenizer, max_length = preprocess_model_inputs(labeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_model(vocab_size, embedding_dim, lstm_units):\n",
    "    input_a = Input(shape=(None,))\n",
    "    input_b = Input(shape=(None,))\n",
    "\n",
    "    # Embedding layer for mapping tokens to vectors\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "    # Shared LSTM layer\n",
    "    shared_lstm = LSTM(lstm_units)\n",
    "\n",
    "    # Process each input through the embedding and LSTM layers\n",
    "    output_a = shared_lstm(embedding_layer(input_a))\n",
    "    output_b = shared_lstm(embedding_layer(input_b))\n",
    "\n",
    "    # Calculate L1 distance between the two representations\n",
    "    l1_distance = Lambda(lambda x: tf.abs(x[0] - x[1]))([output_a, output_b])\n",
    "\n",
    "    # Dense layer to make the final prediction\n",
    "    dense1 = Dense(50, activation='relu')(l1_distance)\n",
    "    droput = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(20, activation='relu')(droput)\n",
    "    prediction = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "    # Create the siamese model\n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "\n",
    "    return siamese_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047/1047 [==============================] - 53s 47ms/step - loss: 0.1529 - accuracy: 0.9454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2549d653f70>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the siamese model\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adjust based on your vocabulary size\n",
    "embedding_dim = 50  # Adjust based on your embedding dimension\n",
    "lstm_units = 100\n",
    "\n",
    "siamese_model = create_siamese_model(vocab_size, embedding_dim, lstm_units)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "siamese_model.fit([padded_company1, padded_company2], y_train.to_numpy(), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_1</th>\n",
       "      <th>entity_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1309879</th>\n",
       "      <td>dolco gmbh</td>\n",
       "      <td>sxp schulz xtruded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148061</th>\n",
       "      <td>hefei rishang electrical appliances coltd</td>\n",
       "      <td>engineered automation systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985823</th>\n",
       "      <td>cambridgeport air systems</td>\n",
       "      <td>cambridgeport air systems inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345376</th>\n",
       "      <td>american tile company</td>\n",
       "      <td>zhejiang xinghuali fiber coltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132154</th>\n",
       "      <td>seyeon inet co</td>\n",
       "      <td>omz pao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          entity_1  \\\n",
       "1309879                                 dolco gmbh   \n",
       "148061   hefei rishang electrical appliances coltd   \n",
       "1985823                  cambridgeport air systems   \n",
       "2345376                      american tile company   \n",
       "1132154                             seyeon inet co   \n",
       "\n",
       "                               entity_2  \n",
       "1309879              sxp schulz xtruded  \n",
       "148061    engineered automation systems  \n",
       "1985823   cambridgeport air systems inc  \n",
       "2345376  zhejiang xinghuali fiber coltd  \n",
       "1132154                         omz pao  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(x_test, threshold=0.5):\n",
    "    \n",
    "    x_test_padded1, x_test_padded2, _, _ = preprocess_model_inputs(x_test)\n",
    "    similarity_score = siamese_model.predict([np.array(x_test_padded1), np.array(x_test_padded2)])\n",
    "\n",
    "    return [int(x) for x in similarity_score >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 4s 4ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 4s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 6s 7ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 5s 5ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 5s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n",
      "938/938 [==============================] - 6s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "start_idx = 0\n",
    "end_idx = 30000\n",
    "pred_list = []\n",
    "while end_idx <= X_test.shape[0]:\n",
    "    y_pred = make_predictions(X_test[start_idx: end_idx])\n",
    "    pred_list.append(y_pred)\n",
    "    start_idx += 30000\n",
    "    end_idx += 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "here we do notice that despite being trained on such a small\n",
    "data set the model has captured the relation ships quite well\n",
    "For experimentation purposes the x_test used is quite larger\n",
    "than the X_train dataset and still the model was able to\n",
    "reproduce a precision of 97% and a recall of 84% which is \n",
    "quite impressive for the initial trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [val for op in pred_list for val in op]\n",
    "y_pred = np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1110000,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test[:y_pred.shape[0]]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767244"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436587"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save_weights(\"siamese_LSTM_embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: siamese_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: siamese_model\\assets\n"
     ]
    }
   ],
   "source": [
    "siamese_model.save(\"siamese_model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further tried handy code snippets for faster functioning\n",
    "\n",
    "# \"\"\" Logic to convert csv data to TFRecords for faster processing in the tfx pipeline\n",
    "# \"\"\"\n",
    "\n",
    "# df_sample.to_csv(Path().cwd() / \"data\" / \"ds_challenge_alpas_sampled.csv\")\n",
    "\n",
    "# import csv\n",
    "# from pathlib import Path\n",
    "# import tensorflow as tf\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def _bytes_feature(value):\n",
    "#     return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "# def _int64_feature(value):\n",
    "#     return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# def clean_rows(row):\n",
    "#     if row[\"tag\"]:\n",
    "#         row[\"tag\"] = int(row[\"tag\"])\n",
    "#     return row\n",
    "\n",
    "\n",
    "# # def convert_zipcode_to_int(zipcode):\n",
    "# #     if isinstance(zipcode, str) and \"XX\" in zipcode:\n",
    "# #         zipcode = zipcode.replace(\"XX\", \"00\")\n",
    "# #     int_zipcode = int(zipcode)\n",
    "# #     return int_zipcode\n",
    "\n",
    "\n",
    "# original_data_file = Path().cwd() / \"data\" / \"ds_challenge_alpas_sampled.csv\"\n",
    "# tfrecords_filename = Path().cwd() / \"tf_records_small\" / \"ds_challenge_alpas_sampled.tfrecords\"\n",
    "# tf_record_writer = tf.io.TFRecordWriter(str(tfrecords_filename))\n",
    "\n",
    "# with open(str(original_data_file), encoding=\"utf8\") as csv_file:\n",
    "#     reader = csv.DictReader(csv_file, delimiter=\",\", quotechar='\"')\n",
    "#     for row in tqdm(reader):\n",
    "#         row = clean_rows(row)\n",
    "#         example = tf.train.Example(\n",
    "#             features=tf.train.Features(\n",
    "#                 feature={\n",
    "#                     \"entity_1\": _bytes_feature(row[\"entity_1\"]),\n",
    "#                     \"entity_2\": _bytes_feature(row[\"entity_2\"]),\n",
    "#                     \"tag\": _int64_feature(row[\"tag\"])\n",
    "#                 }\n",
    "#             )\n",
    "#         )\n",
    "#         tf_record_writer.write(example.SerializeToString())\n",
    "#     tf_record_writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
